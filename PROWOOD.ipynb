{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROWOOD Scraper (Voilà-friendly per Binder)\n",
    "# - Prompt: quante Moderne, quante Classiche, quanti Arredi\n",
    "# - Moderne: https://prowoodsrl.it/cucine-moderne\n",
    "# - Classiche: https://prowoodsrl.it/cucine-classiche/\n",
    "#   -> seleziona random N link 'a.zolo_portfolio_link'\n",
    "#   -> in pagina: immagini da 'li.slick-slide:not(.slick-cloned) img' (usa solo src)\n",
    "#      descrizione = primo <p> dentro '.wpb_text_column'\n",
    "# - Arredi: 7 pagine -> raccoglie prodotti come caroselli '.vc_slide.vc_images_carousel'\n",
    "#      per ogni prodotto scarica TUTTE le immagini in '.vc_item img' (usa solo src)\n",
    "#      nome prodotto dall'H3 in '.zolo_heading_element' più vicino precedente (fallback title/slug)\n",
    "# - ZIP finale con tre cartelle: cucine_moderne, cucine_classiche, arredi\n",
    "# - Controlli download robusti: FileDownload + data-URI + link /voila/files e ../files\n",
    "\n",
    "import os, re, io, time, json, shutil, base64, random\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# ---------- Config ----------\n",
    "USER_AGENT = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123 Safari/537.36 (+educational scraper)\"\n",
    "REQUEST_TIMEOUT = 30\n",
    "DELAY_SEC = 0.6  # cortesia\n",
    "\n",
    "URL_MODERNE   = \"https://prowoodsrl.it/cucine-moderne\"\n",
    "URL_CLASSICHE = \"https://prowoodsrl.it/cucine-classiche/\"\n",
    "ARREDI_PAGINE = [\n",
    "    \"https://prowoodsrl.it/zt_portfolio/pianca/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/connubia/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/baxar/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/neff/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/turati-t4/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/barzaghi/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/radice-mobili/\",\n",
    "]\n",
    "\n",
    "BTN_STYLE = \"display:inline-block;padding:10px 18px;background:#1e7e34;color:#fff;text-decoration:none;border-radius:8px;font-weight:600;font-size:15px;border:1px solid #17642a;\"\n",
    "\n",
    "# ---------- HTTP session ----------\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": USER_AGENT, \"Accept-Language\": \"it-IT,it;q=0.9\"})\n",
    "\n",
    "def fetch_soup(url: str) -> BeautifulSoup:\n",
    "    r = session.get(url, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def slugify(text: str, maxlen=90) -> str:\n",
    "    text = (text or \"\").strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.replace(\"/\", \"-\").replace(\"\\\\\", \"-\")\n",
    "    text = re.sub(r\"[^0-9A-Za-zÀ-ÖØ-öø-ÿ _\\-\\.\\(\\)]\", \"\", text)\n",
    "    text = text[:maxlen]\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text or \"senza_nome\"\n",
    "\n",
    "def ensure_dir(path: str) -> str:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def download_file(url: str, dest_path: str, retries=2) -> bool:\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            with session.get(url, stream=True, timeout=REQUEST_TIMEOUT) as r:\n",
    "                r.raise_for_status()\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                with open(dest_path, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(64 * 1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return True\n",
    "        except Exception:\n",
    "            if attempt == retries:\n",
    "                return False\n",
    "            time.sleep(0.8)\n",
    "\n",
    "# helper per riconoscere solo immagini “reali” della media library\n",
    "def _looks_upload(u: str) -> bool:\n",
    "    return bool(re.search(r\"/wp-content/uploads/.*\\.(jpe?g|png|webp)(\\?.*)?$\", u, re.I))\n",
    "\n",
    "# ---------- Parsers Prowood ----------\n",
    "def collect_portfolio_links(list_url: str) -> list:\n",
    "    \"\"\"\n",
    "    Dalla pagina 'cucine-*' raccoglie tutti i link 'a.zolo_portfolio_link' (assoluti).\n",
    "    \"\"\"\n",
    "    soup = fetch_soup(list_url)\n",
    "    out, seen = [], set()\n",
    "    for a in soup.select(\"a.zolo_portfolio_link[href]\"):\n",
    "        href = urljoin(list_url, a.get(\"href\"))\n",
    "        if href not in seen:\n",
    "            seen.add(href); out.append(href)\n",
    "    return out\n",
    "\n",
    "def parse_kitchen_detail(detail_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    In pagina cucina:\n",
    "    - nome: H1 se presente, altrimenti <title>, altrimenti slug dell'URL\n",
    "    - immagini: SOLO da 'src'\n",
    "        1) prova: 'li.slick-slide:not(.slick-cloned) img'\n",
    "        2) fallback: <img> in sezioni tipiche (.portfolio_single, .zt-portfolio-single, .single-portfolio, .wpb_gallery, .content-area)\n",
    "           filtrando solo path in /wp-content/uploads/\n",
    "           (riordina dando priorità a URL con 'slider')\n",
    "           🔹 ESCLUDE quelle dentro .related_portfolio_list\n",
    "    - descrizione: primo <p> dentro '.wpb_text_column'\n",
    "    \"\"\"\n",
    "    soup = fetch_soup(detail_url)\n",
    "\n",
    "    # ---- nome\n",
    "    h1 = soup.select_one(\"h1\")\n",
    "    if h1 and h1.get_text(strip=True):\n",
    "        name = h1.get_text(strip=True)\n",
    "    elif soup.title and soup.title.string:\n",
    "        name = soup.title.string.strip()\n",
    "    else:\n",
    "        name = urlparse(detail_url).path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    name = slugify(name)\n",
    "\n",
    "    # ---- immagini (solo src) - tentativo slider “canonico”\n",
    "    img_urls, seen = [], set()\n",
    "    for img in soup.select(\"li.slick-slide:not(.slick-cloned) img\"):\n",
    "        src = img.get(\"src\")\n",
    "        if not src or src.startswith(\"data:\"):\n",
    "            continue\n",
    "        absu = urljoin(detail_url, src)\n",
    "        if _looks_upload(absu) and absu not in seen:\n",
    "            seen.add(absu); img_urls.append(absu)\n",
    "\n",
    "    # ---- fallback se lo slider è vuoto\n",
    "    if not img_urls:\n",
    "        containers = soup.select(\n",
    "            \".portfolio_single, .zt-portfolio-single, .single-portfolio, .wpb_gallery, .content-area\"\n",
    "        ) or [soup]\n",
    "\n",
    "        slider_like = re.compile(r\"slider\", re.I)\n",
    "\n",
    "        for cont in containers:\n",
    "            for img in cont.select(\"img\"):\n",
    "                src = img.get(\"src\")\n",
    "                if not src or src.startswith(\"data:\"):\n",
    "                    continue\n",
    "\n",
    "                # 🔹 escludi immagini dentro .related_portfolio_list\n",
    "                if img.find_parent(\".related_portfolio_list\"):\n",
    "                    continue\n",
    "\n",
    "                absu = urljoin(detail_url, src)\n",
    "                # prendi solo immagini della media library WP\n",
    "                if _looks_upload(absu) and absu not in seen:\n",
    "                    seen.add(absu); img_urls.append(absu)\n",
    "\n",
    "        # (facoltativo) privilegia le immagini con 'slider' nel percorso\n",
    "        img_urls.sort(key=lambda u: (0 if slider_like.search(u) else 1, u))\n",
    "\n",
    "    # ---- descrizione\n",
    "    descr = \"\"\n",
    "    text_col = soup.select_one(\".wpb_text_column\")\n",
    "    if text_col:\n",
    "        p = text_col.find(\"p\")\n",
    "        if p:\n",
    "            descr = p.get_text(\" \", strip=True)\n",
    "\n",
    "    return {\"name\": name, \"images\": img_urls, \"description\": descr}\n",
    "\n",
    "def collect_arredi_products(pages: list) -> list:\n",
    "    \"\"\"\n",
    "    Raccoglie PRODOTTI (non singole immagini) dalle pagine arredi.\n",
    "    Per ogni '.vc_slide.vc_images_carousel' costruisce:\n",
    "      { \"name\": <nome_prodotto>, \"images\": [url1, url2, ...] }\n",
    "\n",
    "    - name: h3 dentro '.zolo_heading_element' più vicino PRIMA del carousel (fallback: title/slug)\n",
    "    - images: tutte le img dai '.vc_item img' dentro il carousel (usa SOLO 'src')\n",
    "    \"\"\"\n",
    "    products = []\n",
    "    for pg in pages:\n",
    "        soup = fetch_soup(pg)\n",
    "\n",
    "        carousels = soup.select(\".vc_slide.vc_images_carousel\")\n",
    "        if not carousels:\n",
    "            carousels = soup.select(\".vc_images_carousel\")\n",
    "\n",
    "        if not carousels:\n",
    "            # fallback: nessun carousel; prova a raggruppare per nome pagina\n",
    "            imgs = []\n",
    "            for img in soup.select(\"img\"):\n",
    "                cand = img.get(\"src\")\n",
    "                if cand:\n",
    "                    imgs.append(urljoin(pg, cand))\n",
    "            title = soup.title.string.strip() if (soup.title and soup.title.string) else urlparse(pg).path.rstrip(\"/\").split(\"/\")[-1]\n",
    "            products.append({\"name\": slugify(title), \"images\": imgs})\n",
    "            continue\n",
    "\n",
    "        for car in carousels:\n",
    "            # Trova heading precedente (fratelli)\n",
    "            product_name = None\n",
    "            prev = car\n",
    "            for _ in range(15):\n",
    "                prev = prev.find_previous_sibling()\n",
    "                if not prev: break\n",
    "                h3 = prev.select_one(\".zolo_heading_element h3\") or prev.find(\"h3\")\n",
    "                if h3 and h3.get_text(strip=True):\n",
    "                    product_name = h3.get_text(strip=True)\n",
    "                    break\n",
    "\n",
    "            # Fallback antenati\n",
    "            if not product_name:\n",
    "                parent = car.parent\n",
    "                hops = 0\n",
    "                while parent and hops < 5:\n",
    "                    h3 = parent.select_one(\".zolo_heading_element h3\") or parent.find(\"h3\")\n",
    "                    if h3 and h3.get_text(strip=True):\n",
    "                        product_name = h3.get_text(strip=True)\n",
    "                        break\n",
    "                    parent = parent.parent\n",
    "                    hops += 1\n",
    "\n",
    "            # Fallback finale\n",
    "            if not product_name:\n",
    "                product_name = soup.title.string.strip() if (soup.title and soup.title.string) else urlparse(pg).path.rstrip(\"/\").split(\"/\")[-1]\n",
    "            product_name = slugify(product_name)\n",
    "\n",
    "            # immagini del prodotto: tutte le .vc_item img (usa SOLO src)\n",
    "            img_urls, seen = [], set()\n",
    "            for img in car.select(\".vc_item img\"):\n",
    "                cand = img.get(\"src\")\n",
    "                if cand:\n",
    "                    absu = urljoin(pg, cand)\n",
    "                    if absu not in seen:\n",
    "                        seen.add(absu); img_urls.append(absu)\n",
    "\n",
    "            products.append({\"name\": product_name, \"images\": img_urls})\n",
    "\n",
    "        time.sleep(DELAY_SEC)\n",
    "    return products\n",
    "\n",
    "# ---------- Download controls (robusti) ----------\n",
    "download_area = widgets.Output()\n",
    "\n",
    "def _voila_prefix():\n",
    "    root = os.environ.get(\"JUPYTERHUB_SERVICE_PREFIX\", \"/\")\n",
    "    if not root.endswith(\"/\"): root += \"/\"\n",
    "    return root\n",
    "\n",
    "def _show_download_controls(zip_path: str):\n",
    "    with download_area:\n",
    "        download_area.clear_output()\n",
    "        name = os.path.basename(zip_path)\n",
    "        size_mb = os.path.getsize(zip_path) / (1024*1024)\n",
    "        print(f\"[INFO] ZIP: {name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "        # 1) FileDownload se disponibile\n",
    "        if hasattr(widgets, \"FileDownload\"):\n",
    "            def _zip_bytes():\n",
    "                with open(zip_path, \"rb\") as f:\n",
    "                    return f.read()\n",
    "            btn = widgets.FileDownload(\n",
    "                data=_zip_bytes,\n",
    "                filename=name,\n",
    "                description=\"⬇️ Scarica ZIP\",\n",
    "                button_style=\"primary\",\n",
    "                icon=\"download\"\n",
    "            )\n",
    "            display(btn)\n",
    "        else:\n",
    "            print(\"[INFO] ipywidgets<8: uso fallback senza FileDownload\")\n",
    "\n",
    "        # 2) data-URI (sempre)\n",
    "        with open(zip_path, \"rb\") as f:\n",
    "            b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "        BTN_STYLE = \"display:inline-block;margin-top:30px;padding:10px 20px;border-radius:25px;background-color:#333333;color:#fff;text-decoration:none;font-weight:bold;font-size:16px;\"\n",
    "        html_data = f'<p><a download=\"{name}\" href=\"data:application/zip;base64,{b64}\" style=\"{BTN_STYLE}\">Scarica ZIP</a></p>'\n",
    "        display(HTML(html_data))\n",
    "\n",
    "        # 3) link /voila/files e relativo ../files\n",
    "        pref = _voila_prefix()\n",
    "        abs_href = f\"{pref}voila/files/{name}\"\n",
    "        rel_href = f\"../files/{name}\"\n",
    "        html_extra = (\n",
    "            f'<p>Altri link (se necessario): '\n",
    "            f'<a href=\"{abs_href}\" target=\"_blank\">{abs_href}</a> | '\n",
    "            f'<a href=\"{rel_href}\" target=\"_blank\">{rel_href}</a></p>'\n",
    "        )\n",
    "        display(HTML(html_extra))\n",
    "\n",
    "# ---------- UI ----------\n",
    "title = widgets.HTML(\"<h3>Scarica cucine moderne/classiche e arredi da prowoodsrl.it</h3>\")\n",
    "moderne_n   = widgets.BoundedIntText(value=2, min=0, max=50, step=1, description=\"Moderne:\")\n",
    "classiche_n = widgets.BoundedIntText(value=2, min=0, max=50, step=1, description=\"Classiche:\")\n",
    "arredi_n    = widgets.BoundedIntText(value=4, min=0, max=200, step=1, description=\"Arredi:\")\n",
    "btn_start   = widgets.Button(description=\"Avvia scraping\", button_style=\"success\", icon=\"play\")\n",
    "out         = widgets.Output()\n",
    "\n",
    "def run_scraper(b):\n",
    "    out.clear_output()\n",
    "    with out:\n",
    "        N_mod = int(moderne_n.value)\n",
    "        N_cla = int(classiche_n.value)\n",
    "        N_arr = int(arredi_n.value)\n",
    "\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        root = ensure_dir(f\"./prowood_export_{ts}\")\n",
    "        dir_mod = ensure_dir(os.path.join(root, \"cucine_moderne\"))\n",
    "        dir_cla = ensure_dir(os.path.join(root, \"cucine_classiche\"))\n",
    "        dir_arr = ensure_dir(os.path.join(root, \"arredi\"))\n",
    "        manifest = {\"moderne\": [], \"classiche\": [], \"arredi\": []}\n",
    "\n",
    "        # --- Moderne ---\n",
    "        try:\n",
    "            links_mod = collect_portfolio_links(URL_MODERNE)\n",
    "            print(f\"[Moderne] trovati {len(links_mod)} link\")\n",
    "            sample_mod = random.sample(links_mod, k=min(N_mod, len(links_mod))) if links_mod else []\n",
    "            for idx, u in enumerate(sample_mod, 1):\n",
    "                info = parse_kitchen_detail(u)\n",
    "                kfolder = ensure_dir(os.path.join(dir_mod, info[\"name\"]))\n",
    "                # descrizione\n",
    "                with open(os.path.join(kfolder, \"descrizione.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(info[\"description\"] or \"\")\n",
    "                # immagini\n",
    "                saved = 0\n",
    "                for j, img in enumerate(info[\"images\"], 1):\n",
    "                    ext = os.path.splitext(urlparse(img).path)[1] or \".jpg\"\n",
    "                    ext = ext[:5]\n",
    "                    dest = os.path.join(kfolder, f\"{j:03d}{ext}\")\n",
    "                    if download_file(img, dest): saved += 1\n",
    "                print(f\"  - {idx}/{len(sample_mod)} {info['name']}: img {saved}/{len(info['images'])}\")\n",
    "                manifest[\"moderne\"].append({\"url\": u, \"name\": info[\"name\"], \"images\": saved})\n",
    "                time.sleep(DELAY_SEC)\n",
    "        except Exception as e:\n",
    "            print(\"[Moderne] ERRORE:\", e)\n",
    "\n",
    "        # --- Classiche ---\n",
    "        try:\n",
    "            links_cla = collect_portfolio_links(URL_CLASSICHE)\n",
    "            print(f\"[Classiche] trovati {len(links_cla)} link\")\n",
    "            sample_cla = random.sample(links_cla, k=min(N_cla, len(links_cla))) if links_cla else []\n",
    "            for idx, u in enumerate(sample_cla, 1):\n",
    "                info = parse_kitchen_detail(u)\n",
    "                kfolder = ensure_dir(os.path.join(dir_cla, info[\"name\"]))\n",
    "                with open(os.path.join(kfolder, \"descrizione.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(info[\"description\"] or \"\")\n",
    "                saved = 0\n",
    "                for j, img in enumerate(info[\"images\"], 1):\n",
    "                    ext = os.path.splitext(urlparse(img).path)[1] or \".jpg\"\n",
    "                    ext = ext[:5]\n",
    "                    dest = os.path.join(kfolder, f\"{j:03d}{ext}\")\n",
    "                    if download_file(img, dest): saved += 1\n",
    "                print(f\"  - {idx}/{len(sample_cla)} {info['name']}: img {saved}/{len(info['images'])}\")\n",
    "                manifest[\"classiche\"].append({\"url\": u, \"name\": info[\"name\"], \"images\": saved})\n",
    "                time.sleep(DELAY_SEC)\n",
    "        except Exception as e:\n",
    "            print(\"[Classiche] ERRORE:\", e)\n",
    "\n",
    "        # --- Arredi ---\n",
    "        try:\n",
    "            products = collect_arredi_products(ARREDI_PAGINE)  # list of {name, images[]}\n",
    "            print(f\"[Arredi] prodotti trovati: {len(products)}\")\n",
    "            sample_arr = random.sample(products, k=min(N_arr, len(products))) if products else []\n",
    "\n",
    "            for idx, prod in enumerate(sample_arr, 1):\n",
    "                pname = prod[\"name\"]\n",
    "                pfolder = ensure_dir(os.path.join(dir_arr, slugify(pname)))\n",
    "                urls = prod[\"images\"]\n",
    "                saved = 0\n",
    "                for j, img in enumerate(urls, 1):\n",
    "                    ext = os.path.splitext(urlparse(img).path)[1] or \".jpg\"\n",
    "                    ext = ext[:5]\n",
    "                    dest = os.path.join(pfolder, f\"{j:03d}{ext}\")\n",
    "                    if download_file(img, dest): saved += 1\n",
    "                print(f\"  - {idx}/{len(sample_arr)} {pname}: img {saved}/{len(urls)}\")\n",
    "                manifest[\"arredi\"].append({\"name\": pname, \"images\": saved})\n",
    "                time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print(\"[Arredi] ERRORE:\", e)\n",
    "\n",
    "        # manifest + zip\n",
    "        with open(os.path.join(root, \"manifest.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        zip_path = shutil.make_archive(root, \"zip\", root)\n",
    "        print(\"\\n[OK] ZIP pronto:\", zip_path)\n",
    "        _show_download_controls(zip_path)\n",
    "\n",
    "btn_start.on_click(run_scraper)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    title,\n",
    "    widgets.HBox([moderne_n, classiche_n, arredi_n]),\n",
    "    btn_start,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    out,\n",
    "    download_area\n",
    "]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
