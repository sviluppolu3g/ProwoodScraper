{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROWOOD SCRAPER NOTEBOOK (Binder/Voila friendly)\n",
    "\n",
    "import os, re, io, time, json, shutil, random, base64\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# ====================== CONFIG ==========================\n",
    "USER_AGENT = \"Mozilla/5.0 (compatible; prowood-scraper/1.0; +https://example.com)\"\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": USER_AGENT, \"Accept-Language\": \"it-IT,it;q=0.9\"})\n",
    "\n",
    "# urls di partenza\n",
    "URL_MODERNE = \"https://prowoodsrl.it/cucine-moderne/\"\n",
    "URL_CLASSICHE = \"https://prowoodsrl.it/cucine-classiche/\"\n",
    "URL_ARREDI = [\n",
    "    \"https://prowoodsrl.it/zt_portfolio/pianca/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/connubia/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/baxar/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/neff/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/turati-t4/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/barzaghi/\",\n",
    "    \"https://prowoodsrl.it/zt_portfolio/radice-mobili/\"\n",
    "]\n",
    "\n",
    "# =================== UTILS =============================\n",
    "def fetch_soup(url):\n",
    "    r = session.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "def slugify(text, maxlen=80):\n",
    "    text = re.sub(r\"\\s+\", \"-\", text.strip())\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\-_]+\", \"\", text)\n",
    "    return (text[:maxlen] or \"item\").strip(\"-_\")\n",
    "\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def download_file(url, dest_path, retries=2):\n",
    "    for attempt in range(retries+1):\n",
    "        try:\n",
    "            with session.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest_path, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return True\n",
    "        except Exception:\n",
    "            if attempt == retries:\n",
    "                return False\n",
    "            time.sleep(1)\n",
    "\n",
    "# -------- funzioni scraping -------------------\n",
    "def _abs(u, base): \n",
    "    return urljoin(base, u) if u else None\n",
    "\n",
    "def _looks_upload(u: str) -> bool:\n",
    "    return bool(re.search(r\"/wp-content/uploads/.*\\.(jpe?g|png|webp)(\\?.*)?$\", u, re.I))\n",
    "\n",
    "def parse_kitchen_detail(detail_url: str) -> dict:\n",
    "    soup = fetch_soup(detail_url)\n",
    "    # name\n",
    "    h1 = soup.select_one(\"h1\")\n",
    "    if h1 and h1.get_text(strip=True):\n",
    "        name = h1.get_text(strip=True)\n",
    "    elif soup.title and soup.title.string:\n",
    "        name = soup.title.string.strip()\n",
    "    else:\n",
    "        name = urlparse(detail_url).path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    name = slugify(name)\n",
    "\n",
    "    seen, img_urls = set(), []\n",
    "    # tentativo slider\n",
    "    for img in soup.select(\"li.slick-slide:not(.slick-cloned) img\"):\n",
    "        src = img.get(\"src\")\n",
    "        if src:\n",
    "            u = _abs(src, detail_url)\n",
    "            if u and _looks_upload(u) and u not in seen:\n",
    "                seen.add(u); img_urls.append(u)\n",
    "    # fallback\n",
    "    if not img_urls:\n",
    "        containers = soup.select(\".portfolio_single, .zt-portfolio-single, .single-portfolio, .wpb_gallery\") or [soup]\n",
    "        slider_like = re.compile(r\"slider\", re.I)\n",
    "        for cont in containers:\n",
    "            for img in cont.select(\"img\"):\n",
    "                src = img.get(\"src\")\n",
    "                if not src:\n",
    "                    continue\n",
    "                u = _abs(src, detail_url)\n",
    "                if not u or u in seen:\n",
    "                    continue\n",
    "                if not _looks_upload(u):\n",
    "                    continue\n",
    "                seen.add(u); img_urls.append(u)\n",
    "        img_urls.sort(key=lambda x: (0 if slider_like.search(x) else 1, x))\n",
    "    # descrizione\n",
    "    descr = \"\"\n",
    "    text_col = soup.select_one(\".wpb_text_column\")\n",
    "    if text_col:\n",
    "        p = text_col.find(\"p\")\n",
    "        if p:\n",
    "            descr = p.get_text(\" \", strip=True)\n",
    "    return {\"name\": name, \"images\": img_urls, \"description\": descr}\n",
    "\n",
    "def parse_arredo_detail(arredo_url: str) -> list:\n",
    "    soup = fetch_soup(arredo_url)\n",
    "    items = []\n",
    "    for block in soup.select(\".vc_slide.vc_images_carousel\"):\n",
    "        # nome prodotto = h3 sopra il blocco\n",
    "        h3 = block.find_previous(\"h3\")\n",
    "        if not h3:\n",
    "            continue\n",
    "        name = slugify(h3.get_text(strip=True))\n",
    "        img_urls, seen = [], set()\n",
    "        for vc_item in block.select(\".vc_item img\"):\n",
    "            src = vc_item.get(\"src\")\n",
    "            if not src: continue\n",
    "            u = _abs(src, arredo_url)\n",
    "            if u not in seen:\n",
    "                seen.add(u); img_urls.append(u)\n",
    "        if img_urls:\n",
    "            items.append({\"name\": name, \"images\": img_urls})\n",
    "    return items\n",
    "\n",
    "def extract_random_links(list_url: str, selector: str, n=5):\n",
    "    soup = fetch_soup(list_url)\n",
    "    links = [urljoin(list_url, a[\"href\"]) for a in soup.select(selector) if a.get(\"href\")]\n",
    "    random.shuffle(links)\n",
    "    return links[:n]\n",
    "\n",
    "# ========== UI PROMPT =========================\n",
    "n_moderne = widgets.IntText(value=2, description='Cucine moderne:')\n",
    "n_classiche = widgets.IntText(value=2, description='Cucine classiche:')\n",
    "n_arredi = widgets.IntText(value=2, description='Arredi:')\n",
    "btn_start = widgets.Button(description=\"Avvia scraping\", button_style=\"success\")\n",
    "download_area = widgets.Output()\n",
    "\n",
    "def _voila_prefix():\n",
    "    return os.environ.get(\"VOILA_BASE_URL\", \"/\")\n",
    "\n",
    "def _show_download_controls(zip_path: str):\n",
    "    with download_area:\n",
    "        download_area.clear_output()\n",
    "        name = os.path.basename(zip_path)\n",
    "        size_mb = os.path.getsize(zip_path)/(1024*1024)\n",
    "        print(f\"[INFO] ZIP: {name} ({size_mb:.2f} MB)\")\n",
    "        # data URI\n",
    "        with open(zip_path, \"rb\") as f:\n",
    "            b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "        html_data = (\n",
    "            f'<p><a download=\"{name}\" href=\"data:application/zip;base64,{b64}\" '\n",
    "            f'style=\"background:#4CAF50;color:white;padding:10px 15px;text-decoration:none;border-radius:5px;\">'\n",
    "            f'⬇️ Scarica ZIP</a></p>'\n",
    "        )\n",
    "        display(HTML(html_data))\n",
    "        # link /voila/files\n",
    "        pref = _voila_prefix()\n",
    "        abs_href = f\"{pref}voila/files/{name}\"\n",
    "        rel_href = f\"../files/{name}\"\n",
    "        html_extra = (\n",
    "            f'<div>Altri link: '\n",
    "            f'<a href=\"{abs_href}\" target=\"_blank\">{abs_href}</a> | '\n",
    "            f'<a href=\"{rel_href}\" target=\"_blank\">{rel_href}</a></div>'\n",
    "        )\n",
    "        display(HTML(html_extra))\n",
    "\n",
    "def scrape_action(b):\n",
    "    clear_output(wait=True)\n",
    "    display(n_moderne, n_classiche, n_arredi, btn_start, download_area)\n",
    "    print(f\"Avvio: {n_moderne.value} moderne, {n_classiche.value} classiche, {n_arredi.value} arredi\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    root_dir = ensure_dir(f\"./prowood_{timestamp}\")\n",
    "    mod_dir = ensure_dir(os.path.join(root_dir, \"Cucine_Moderne\"))\n",
    "    class_dir = ensure_dir(os.path.join(root_dir, \"Cucine_Classiche\"))\n",
    "    arr_dir = ensure_dir(os.path.join(root_dir, \"Arredi\"))\n",
    "\n",
    "    # cucine moderne\n",
    "    links_mod = extract_random_links(URL_MODERNE, \"a.zolo_portfolio_link\", n_moderne.value)\n",
    "    for idx,u in enumerate(links_mod,1):\n",
    "        data = parse_kitchen_detail(u)\n",
    "        fdir = ensure_dir(os.path.join(mod_dir, f\"{idx:02d}_{data['name']}\"))\n",
    "        with open(os.path.join(fdir, \"descrizione.txt\"),\"w\",encoding=\"utf-8\") as f: f.write(data[\"description\"] or \"\")\n",
    "        for j,img in enumerate(data[\"images\"],1):\n",
    "            ext = os.path.splitext(urlparse(img).path)[1] or \".jpg\"\n",
    "            download_file(img, os.path.join(fdir,f\"img_{j:03d}{ext}\"))\n",
    "\n",
    "    # cucine classiche\n",
    "    links_class = extract_random_links(URL_CLASSICHE, \"a.zolo_portfolio_link\", n_classiche.value)\n",
    "    for idx,u in enumerate(links_class,1):\n",
    "        data = parse_kitchen_detail(u)\n",
    "        fdir = ensure_dir(os.path.join(class_dir, f\"{idx:02d}_{data['name']}\"))\n",
    "        with open(os.path.join(fdir, \"descrizione.txt\"),\"w\",encoding=\"utf-8\") as f: f.write(data[\"description\"] or \"\")\n",
    "        for j,img in enumerate(data[\"images\"],1):\n",
    "            ext = os.path.splitext(urlparse(img).path)[1] or \".jpg\"\n",
    "            download_file(img, os.path.join(fdir,f\"img_{j:03d}{ext}\"))\n",
    "\n",
    "    # arredi\n",
    "    all_arredi=[]\n",
    "    for url in URL_ARREDI:\n",
    "        all_arredi.extend(parse_arredo_detail(url))\n",
    "    random.shuffle(all_arredi)\n",
    "    for idx,item in enumerate(all_arredi[:n_arredi.value],1):\n",
    "        fdir = ensure_dir(os.path.join(arr_dir, f\"{idx:02d}_{item['name']}\"))\n",
    "        for j,img in enumerate(item[\"images\"],1):\n",
    "            ext = os.path.splitext(urlparse(img).path)[1] or \".jpg\"\n",
    "            download_file(img, os.path.join(fdir,f\"img_{j:03d}{ext}\"))\n",
    "\n",
    "    zip_path = shutil.make_archive(root_dir,\"zip\",root_dir)\n",
    "    print(f\"[OK] ZIP pronto: {zip_path}\")\n",
    "    _show_download_controls(zip_path)\n",
    "\n",
    "btn_start.on_click(scrape_action)\n",
    "\n",
    "display(n_moderne, n_classiche, n_arredi, btn_start, download_area)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
